{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    region_name       date  hospitalized_new  symptom:Angular cheilitis  \\\n",
      "0        Hawaii 2020-03-09               0.0                 373.072786   \n",
      "1        Hawaii 2020-03-16               0.0                 330.136609   \n",
      "2        Hawaii 2020-03-23              12.0                 363.979065   \n",
      "3        Hawaii 2020-03-30               7.0                 453.419840   \n",
      "4        Hawaii 2020-04-06              25.0                 417.390287   \n",
      "..          ...        ...               ...                        ...   \n",
      "227     Wyoming 2020-08-24               8.0                   9.934225   \n",
      "228     Wyoming 2020-08-31               4.0                   9.934225   \n",
      "229     Wyoming 2020-09-07               9.0                  10.300463   \n",
      "230     Wyoming 2020-09-14              15.0                   9.934225   \n",
      "231     Wyoming 2020-09-21              19.0                   9.934225   \n",
      "\n",
      "     symptom:Aphonia  symptom:Crackles  symptom:Dysautonomia  \\\n",
      "0         739.814500        363.518623            156.780356   \n",
      "1         578.544838        434.771957            156.780356   \n",
      "2         418.656502        492.787595            156.780356   \n",
      "3         327.489070        415.663631            156.780356   \n",
      "4         265.559678        306.654089            156.780356   \n",
      "..               ...               ...                   ...   \n",
      "227         9.934225          9.934225             15.427805   \n",
      "228         9.934225         13.047254             12.268996   \n",
      "229         9.934225         12.589455             13.367712   \n",
      "230         9.934225          9.934225             14.512208   \n",
      "231         9.934225         11.536519             16.297622   \n",
      "\n",
      "     symptom:Hemolysis  symptom:Rectal pain  symptom:Shallow breathing  \\\n",
      "0           156.780356           156.780356                 570.717331   \n",
      "1           156.780356           156.780356                 964.279767   \n",
      "2           156.780356           156.780356                 850.320478   \n",
      "3           156.780356           156.780356                 692.849459   \n",
      "4           156.780356           156.780356                 512.241252   \n",
      "..                 ...                  ...                        ...   \n",
      "227           9.934225             9.934225                   9.934225   \n",
      "228           9.934225            12.040097                  13.276153   \n",
      "229          11.582299             9.934225                  10.071564   \n",
      "230           9.934225            11.261840                   9.934225   \n",
      "231          20.051569             9.934225                   9.934225   \n",
      "\n",
      "     symptom:Ventricular fibrillation  \n",
      "0                          156.780356  \n",
      "1                          156.780356  \n",
      "2                          156.780356  \n",
      "3                          156.780356  \n",
      "4                          156.780356  \n",
      "..                                ...  \n",
      "227                         11.353400  \n",
      "228                          9.934225  \n",
      "229                          9.934225  \n",
      "230                          9.934225  \n",
      "231                         14.786887  \n",
      "\n",
      "[232 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read CSV dataset files from current directory into Numpy arrays\n",
    "hospitalization_df = pd.read_csv('aggregated_cc_by.csv',sep = ',',low_memory=False)\n",
    "search_trends_df = pd.read_csv('2020_US_weekly_symptoms_dataset.csv',sep = ',',low_memory=False)\n",
    "\n",
    "#Isolate Hospitalization data pertaining to US States in Search Trends dataset\n",
    "hospitalization_df=hospitalization_df[hospitalization_df['open_covid_region_code'].isin(search_trends_df['open_covid_region_code'].unique())]\n",
    "\n",
    "#Sort Hospitalization dataset based on state then date\n",
    "hospitalization_df=hospitalization_df.sort_values(by=['open_covid_region_code','date'])\n",
    "\n",
    "#Remove data before 2020-03-09 in the Symptom Searches dataset in order to merge both datasets\n",
    "#Note that 2020-03-09 is the first Monday which has data in both datasets for the US regions\n",
    "search_trends_df=search_trends_df[search_trends_df['date'] >= \"2020-03-09\"]\n",
    "\n",
    "#Remove data before 2020-03-09 in the Hospitalization dataset in order to merge both datasets\n",
    "#Note that 2020-03-09 is the first Monday which has data in both datasets for the US regions\n",
    "hospitalization_df=hospitalization_df[hospitalization_df['date'] >= \"2020-03-09\"]\n",
    "\n",
    "#Remove data after 2020-09-27 (last Sunday in both datasets) from both datasets\n",
    "#in order to merge both datasets on a weekly basis\n",
    "hospitalization_df=hospitalization_df[hospitalization_df['date'] <= \"2020-09-27\"]\n",
    "search_trends_df=search_trends_df[search_trends_df['date'] <= \"2020-09-27\"]\n",
    "\n",
    "#Delete columns from Hospitalization and Search Trends data that contain only NaN values\n",
    "hospitalization_df=hospitalization_df.dropna(how='all', axis=1)\n",
    "search_trends_df=search_trends_df.dropna(how='all', axis=1)\n",
    "\n",
    "#Isolate data for each Sunday of week (weekly cumulative total of Covid Hospitalizations is at the end of the week)\n",
    "sunday_data=hospitalization_df[pd.to_datetime(hospitalization_df['date']).dt.dayofweek==6].copy(deep=True)\n",
    "\n",
    "#Replace daily data in Hospitalization dataset with the Monday date of the corresponding week\n",
    "hospitalization_df['date']=pd.to_datetime(hospitalization_df['date']) - pd.to_timedelta(pd.to_datetime(hospitalization_df['date']).dt.dayofweek, unit='d')\n",
    "\n",
    "#Add up daily Covid cases to get weekly count\n",
    "sum_weekly_data=hospitalization_df.groupby(['open_covid_region_code','date']).sum().reset_index()['hospitalized_new']\n",
    "sunday_data=sunday_data.reset_index().assign(hospitalized_new=sum_weekly_data)\n",
    "sunday_data['date']=pd.to_datetime(sunday_data['date']) - pd.to_timedelta(pd.to_datetime(sunday_data['date']).dt.dayofweek, unit='d')\n",
    "hospitalization_df=sunday_data\n",
    "\n",
    "#Merge data\n",
    "merged_data_df = pd.concat([hospitalization_df.reset_index(drop=True),search_trends_df.reset_index(drop=True)],axis=1)\n",
    "\n",
    "#Delete duplicate columns and unnecessary columns\n",
    "merged_data_df = merged_data_df.loc[:,~merged_data_df.columns.duplicated()]\n",
    "merged_data_df=merged_data_df.drop(['index','sub_region_1','open_covid_region_code','hospitalized_cumulative','country_region_code','country_region','sub_region_1_code'], axis=1)\n",
    "\n",
    "#Remove columns with more than 35% NaN values\n",
    "threshold=0.35\n",
    "missings = merged_data_df.isna().sum()/merged_data_df.shape[0]\n",
    "deleting = merged_data_df.columns[missings>threshold]\n",
    "cols = merged_data_df.columns[missings<=threshold]\n",
    "merged_data_df.drop(deleting,axis=1,inplace=True)\n",
    "\n",
    "symptoms = merged_data_df.columns[3:]\n",
    "filldf = pd.DataFrame(data=np.zeros(merged_data_df.shape),columns=merged_data_df.columns)\n",
    "themin = np.inf\n",
    "\n",
    "for state in merged_data_df['region_name'].unique():\n",
    "    location = merged_data_df['region_name']==state, symptoms\n",
    "    k = merged_data_df.loc[location]\n",
    "    #Minimum accross all symptom trends in specific US region\n",
    "    min2 = np.min(np.min(k))\n",
    "    if min2<themin:\n",
    "        #Minimum accross all symptom trends and US regions\n",
    "        themin = min2\n",
    "    filldf.loc[location] = min2\n",
    "\n",
    "#Fill NaN values in each column with column minimum\n",
    "merged_data_df.fillna(filldf,inplace=True)\n",
    "if merged_data_df.isnull().values.any():\n",
    "    merged_data_df.fillna(themin,inplace=True)   \n",
    "    \n",
    "#Delete states with 0 hospitalizations\n",
    "states = merged_data_df['region_name'].unique()\n",
    "threshold_on_0=0.8\n",
    "for state in states:\n",
    "    hospitalized_in_state = merged_data_df.loc[merged_data_df['region_name']==state,'hospitalized_new']\n",
    "    hospitalized_count = len(hospitalized_in_state[hospitalized_in_state==0].values)\n",
    "    if hospitalized_count>hospitalized_in_state.shape[0]*threshold_on_0:\n",
    "        merged_data_df.drop(merged_data_df[merged_data_df['region_name']==state].index,axis=0,inplace=True) \n",
    "                \n",
    "#We will take into account internet population\n",
    "#because the data only makes sense in one state, it is comparable. If we want to \n",
    "#compare between states, we need to scale them.\n",
    "internet_usage_df = pd.read_excel('internet_usage.xlsx')\n",
    "internet_usage_df.drop(['total population', 'percentage of internet users'],axis=1, inplace=True)\n",
    "states = merged_data_df['region_name'].unique()\n",
    "internet_usage_df.rename(columns={ internet_usage_df.columns[0]: \"states\" }, inplace = True)\n",
    "truncated_internet_usage_df = internet_usage_df.loc[internet_usage_df['states'].isin(states)]\n",
    "cols = merged_data_df.columns[3:]\n",
    "for state in truncated_internet_usage_df['states']:\n",
    "    merged_data_df.loc[merged_data_df['region_name']==state, cols] *= float(truncated_internet_usage_df[truncated_internet_usage_df['states']==state]['No. internet users']/100000)\n",
    "\n",
    "#States like Nebraska has a crazy increases in hospitalization_new at some points\n",
    "#Here we remove those States from the dataset\n",
    "for state in ['Nebraska','New Mexico','Rhode Island']:\n",
    "    indexes = merged_data_df[merged_data_df['region_name']==state].index\n",
    "    merged_data_df.drop(indexes,inplace=True)\n",
    "\n",
    "#Reset indices\n",
    "merged_data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Generate CSV file from dataframe\n",
    "merged_data_df.to_csv('merged_data.csv', encoding='utf-8')\n",
    "\n",
    "print(merged_data_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
